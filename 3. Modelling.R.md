## Load preprocessed data

```{r}
load("Data preprocessed.RData")
data <- data_preprocessed
rm(data_preprocessed)
```

## Activate parallelization

```{r}
max_threads <- 6
cluster <- parallel::makeCluster(max_threads)
doParallel::registerDoParallel(cluster)
```

## Define performance metrics

```{r}
mse <- function(true_values, predictions){return(mean((true_values-predictions)^2))}
spearman <- function(true_values, predictions){return(cor(true_values, predictions, method="spearman"))}
```

## Resume of features

```{r}
ids <- c("GolferID","Week")
target <- "RankingPoints"
features <- names(data)[!(names(data) %in% c(ids,target))]
```

## Defining candidate parametrizations of XGBoost

```{r}
max_depth <- c(15,30)
nrounds <- c(250,500)
eta <- c(0.001,0.01,0.05)
subsample <- c(0.5,1)
colsample_bytree <-  c(0.5,0.75,1)
parametrizations <- expand.grid(max_depth, nrounds, eta, subsample, colsample_bytree)
names(parametrizations) <- c("max_depth","nrounds","eta","subsample","colsample_bytree")
```

## Auxiliar function for optimal parametrization search in parallel with back-testing validation

This auxiliary function does back-testing validation for one given parametrization of XGBoost model. Basically, it returns the metrics values MSE and Spearman correlation for the 150 most recent weeks in the preprocessed data (out of approximately 600) using until the respective previous week for training the model and the week in question (plus the following 49) as validation set (simulating a real new tournaments prediction). It is like a rolling window but with window size 50 weeks (for avoiding training the same model several times with only few weeks more data). Note: approximately 50 weeks contain 10% of total preprocessed data.
```{r}
number_weeks_val <- 150
weeks_val <- unique(data$Week)[1:number_weeks_val]
weeks_val_from_old_to_recent <- weeks_val[order(-weeks_val)]
save(weeks_val_from_old_to_recent, data, features, target, mse, spearman, parametrizations, file="tmp.RData")
auxiliary <- function(p){
  print("Starting parametrization")
  load("tmp.RData")
  library(xgboost)
  mse_by_weeks <- c()
  spearman_by_weeks <- c()
  for(index in seq(1, length(weeks_val_from_old_to_recent), 50)){
    train <- data[data$Week>weeks_val_from_old_to_recent[index],]
    train_xgbDMatrix <- xgb.DMatrix(data=data.matrix(train[,features]), label=train[,target])
    set.seed(1)
    model <- xgboost(data=train_xgbDMatrix,
                     max_depth=parametrizations[p,"max_depth"],
                     nrounds=parametrizations[p,"nrounds"],
                     eta=parametrizations[p,"eta"],
                     subsample=parametrizations[p,"subsample"],
                     colsample_bytree=parametrizations[p,"colsample_bytree"],
                     verbose=FALSE,
                     missing=-9999)
    for(week in weeks_val_from_old_to_recent[index:(index+49)]){
      val <- data[data$Week==week,]
      val_xgbDMatrix <- xgb.DMatrix(data=data.matrix(val[,features]))
      val_prediction <- predict(model, val_xgbDMatrix)
      mse_by_weeks <- c(mse_by_weeks, mse(val[,target], val_prediction))
      spearman_by_weeks <- c(spearman_by_weeks, spearman(val[,target], val_prediction))
    }
  }
  return(list(mse_by_weeks, spearman_by_weeks))
}
```

## Optimal parametrization search with back-testing validation

Running the optimal parametrization search with back-testing validation in parallel:
```{r}
start <- Sys.time()
search_results <- plyr::llply(1:nrow(parametrizations), auxiliary, .parallel=TRUE)
end <- Sys.time()
end-start
file.remove("tmp.Rdata")
```

Saving the metrics results by validation weeks from the list of all parametrizations searched/tried in parallel:
```{r}
weeks_names <- paste0("Week_", weeks_val_from_old_to_recent)
parametrizations[,weeks_names] <- NA
validation_results_mse <- parametrizations
validation_results_spearman <- parametrizations
for(p in 1:nrow(parametrizations)){
  validation_results_mse[p,weeks_names] <- search_results[[p]][[1]]
  validation_results_spearman[p,weeks_names] <- search_results[[p]][[2]]
}
validation_results_mse[,"Mean"] <- apply(validation_results_mse[,weeks_names], 1, mean)
validation_results_mse[,"Variance"] <- apply(validation_results_mse[,weeks_names], 1, var)
validation_results_spearman[,"Mean"] <- apply(validation_results_spearman[,weeks_names], 1, mean)
validation_results_spearman[,"Variance"] <- apply(validation_results_spearman[,weeks_names], 1, var)
save(validation_results_mse, validation_results_spearman, file="Results backtesting validation.RData")
```

## Train and save final optimal model

The chosen parametrization with the best trade-off of low mean and low variance of MSE in validation weeks is ???: *max_depth*=, *nrounds*=, *eta*=, *subsample*=, *colsample_bytree*=, *missing*=-9999.

The chosen parametrization with the best trade-off of high mean and low variance of Spearman correlation in validation weeks is ???: *max_depth*=, *nrounds*=, *eta*=, *subsample*=, *colsample_bytree*=, *missing*=-9999.

In total 4 final models because I will train, for each optimal parametrization by metric, a model with all data available and other without the validation weeks (150 most recent ones). It is useful for testing and production purposes.

```{r}
library(xgboost)
train_all_xgbDMatrix <- xgb.DMatrix(data=data.matrix(data[,features]), label=data[,target])
train_wo_val <- data[data$Week>weeks_val_from_old_to_recent[1],]
train_wo_val_xgbDMatrix <- xgb.DMatrix(data=data.matrix(train_wo_val[,features]), label=train_wo_val[,target])
best_parameters_mse <-
best_parameters_spearman <-
# A: Best MSE parametrization and all data
set.seed(1)
model_mse_all <- xgboost(data=train_all_xgbDMatrix,
                         max_depth=parametrizations[best_parameters_mse,"max_depth"],
                         nrounds=parametrizations[best_parameters_mse,"nrounds"],
                         eta=parametrizations[best_parameters_mse,"eta"],
                         subsample=parametrizations[best_parameters_mse,"subsample"],
                         colsample_bytree=parametrizations[best_parameters_mse,"colsample_bytree"],
                         verbose=FALSE,
                         missing=-9999)
xgb.save(model_mse_all, "Model best MSE with all data.model")
# B: Best Spearman parametrization and all data
set.seed(1)
model_spearman_all <- xgboost(data=train_all_xgbDMatrix,
                              max_depth=parametrizations[best_parameters_spearman,"max_depth"],
                              nrounds=parametrizations[best_parameters_spearman,"nrounds"],
                              eta=parametrizations[best_parameters_spearman,"eta"],
                              subsample=parametrizations[best_parameters_spearman,"subsample"],
                              colsample_bytree=parametrizations[best_parameters_spearman,"colsample_bytree"],
                              verbose=FALSE,
                              missing=-9999)
xgb.save(model_spearman_all, "Model best Spearman with all data.model")
# C: Best MSE parametrization without validation data
set.seed(1)
model_mse_wo_val <- xgboost(data=train_wo_val_xgbDMatrix,
                            max_depth=parametrizations[best_parameters_mse,"max_depth"],
                            nrounds=parametrizations[best_parameters_mse,"nrounds"],
                            eta=parametrizations[best_parameters_mse,"eta"],
                            subsample=parametrizations[best_parameters_mse,"subsample"],
                            colsample_bytree=parametrizations[best_parameters_mse,"colsample_bytree"],
                            verbose=FALSE,
                            missing=-9999)
xgb.save(model_mse_wo_val, "Model best MSE without validation data.model")
# D: Best Spearman parametrization without validation data
set.seed(1)
model_spearman_wo_val <- xgboost(data=train_wo_val_xgbDMatrix,
                                 max_depth=parametrizations[best_parameters_spearman,"max_depth"],
                                 nrounds=parametrizations[best_parameters_spearman,"nrounds"],
                                 eta=parametrizations[best_parameters_spearman,"eta"],
                                 subsample=parametrizations[best_parameters_spearman,"subsample"],
                                 colsample_bytree=parametrizations[best_parameters_spearman,"colsample_bytree"],
                                 verbose=FALSE,
                                 missing=-9999)
xgb.save(model_spearman_wo_val, "Model best Spearman without validation data.model")
```
## Post analyis

Using the 2 final models trained without validation data, let's compute the % of real top 7 golfers (more RankingPoints) of each validation week that fall in the predicted top 7:
```{r}
well_matched <- data.frame(matrix(nrow=length(weeks_val_from_old_to_recent),ncol=6))
names(percentage_well_matched_ranks) <- paste0(paste0("Top ", c(3,5,7,10)), c(" MSE"," Spearman"))
rownames(percentage_well_matched_ranks) <- paste0("Week ", weeks_val_from_old_to_recent)
for(week in weeks_val_from_old_to_recent){
  data_week <- data[data$Week==week,]
  data_week_xgbDMatrix <- xgb.DMatrix(data=data.matrix(data[,features]))
  true_values_week <- data_week[,target]
  rank_true <- order(-true_values_week)
  predictions_week_model_mse_wo_val <- predict(model_mse_wo_val, data_week_xgbDMatrix)
  rank_model_mse_wo_val <- order(-predictions_week_model_mse_wo_val)
  predictions_week_model_spearman_wo_val <- predict(model_spearman_wo_val, data_week_xgbDMatrix)
  rank_model_spearman_wo_val <- order(-predictions_week_model_spearman_wo_val)
  for(top in c(3,5,7,10)){
    row <- paste0("Week ", week)
    column_mse <- paste0("Top ", top, " MSE")
    column_spearman <- paste0("Top ", top, " Spearman")
    well_matched[row,column_mse] <- mean(rank_true[1:top] %in% rank_model_mse_wo_val[1:top])
    well_matched[row,column_spearman] <- mean(rank_true[1:top] %in% rank_model_spearman_wo_val[1:top])
  }
}
round(well_matched*100, 2)
```